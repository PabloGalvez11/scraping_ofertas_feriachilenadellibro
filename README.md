#  Proyecto de Web Scraping y an谩lisis de datos - ofertas de la p谩gina de la librer铆a Feria Chilena del libro ("https://feriachilenadellibro.cl") 

Este proyecto consiste en la aplicaci贸n en conjunto de diversas t茅cnicas aprendidas para la elaboraci贸n de un pipeline a partir de datos reales extra铆dos de la p谩gina web de la librer铆a Feria Chilena del Libro, concretamente de las ofertas ofrecidas v铆a internet.

El proyecto tiene por objetivo abarcar y practicar el uso de diversas habilidades que puedan realizarse con s贸lo poseer un nicho de datos. 

## Estructura del proyecto

### Carpetas del proyecto

- **data/raw** : Contiene los datos obtenidos mediante web scraping.
- **data/processed** : Contiene los datos de raw luego de haberles realizado.
- **deployments**: Contiene el c贸digo para la creaci贸n de una web API.
- **models**: Contiene los tres tipos de modelos separados en carpetas seg煤n la variable objetivo.
- **notebooks**: Contiene los c贸digos "prototipo" de los pasos previos a la estructuraci贸n en archivos .py.
- **paletas de colores**: Contiene paletas de colores para ser utilizadas en PowerBI.
- **queries**: Contiene las queries que se realizaron para la creaci贸n de la base de datos en PostgreSQL.
- **src/pipeline**: Contiene los archivos .py utilizados para la elaboraci贸n del pipeline.
- **words**: Contiene documentos de texto word con c贸digos para la creaci贸n de medidas y columnas DAX, adem谩s de preguntas para responder con gr谩ficas de PowerBI.



### Archivos dentro de src/pipeline
- **__init__.py**: Se utiliza para asignar la carpeta de pipeline como un paquete.
- **config.py**: Contiene variables y funciones definidas para proveer a los dem谩s archivos.
- **guardar_en_SQL.py**: Contiene el c贸digo para el guardado de datasets dentro de la base de datos.
- **flow.py**: Contiene el flujo para el funcionamiento del pipeline.
- **limpieza_datos.py**: Contiene el proceso de limpieza de datos de los datos extra铆dos por medio de web scraping.
- **modelo_descuento.py**: Contiene el proceso para el entrenamiento, predicci贸n y exportaci贸n del modelo relacionado a predecir el porcentaje de descuento de un libro.
- **modelo_precio_oferta.py**: Contiene el proceso para el entrenamiento, predicci贸n y exportaci贸n del modelo relacionado a predecir el precio de oferta de un libro.
- **modelo_precio_original.py**: Contiene el proceso para el entrenamiento, predicci贸n y exportaci贸n del modelo relacionado a predecir el precio original de un libro.
- **scraping.py**: Contiene el proceso de extracci贸n de datos mediante web scraping.

### Archivos adicionales
- **.env**: Se utiliza para guardar las credenciales de la base de datos.
- **.env.example**: Se utiliza para que otras personas puedan realizar el proceso sin utilizar mis mismas credenciales de la base de datos.
- **requirements.txt**: Contiene todas las librer铆as utilizadas dentro del proyecto, algunas con sus respectivas versiones.

## Metodolog铆a

Se contemplan los siguientes pasos generales dentro del pipeline para la creaci贸n de un flujo para el proceso de extracci贸n, limpieza, an谩lisis predictivo, NLP y guardado de datos:

---

### 0. Definir variables y directorios a utilizar

Se realiz贸 dentro de `config.py`. Este archivo contiene variables y funciones definidas para proveer a los dem谩s archivos, tales como la fecha de extracci贸n de datos (fecha de hoy), detalles de la base de datos local, directorio de carpetas y funci贸n para estandarizaci贸n. Si bien, no se puede considerar un paso como tal, es crucial su inclusi贸n previa a los otros pasos.

###  1. Web Scraping para extracci贸n de datos

Se realiz贸 dentro de `scraping.py`, principalmente con uso de la librer铆a `BeautifulSoup`. El proceso consiste en:
- Identificar la cantidad de pesta帽as dentro de la secci贸n de ofertas, para luego solicitar al usuario entre qu茅 pesta帽as realizar la extracci贸n 
- Ingresar uno por uno al link de cada libro dentro de la secci贸n de ofertas, para luego extraer informaci贸n m谩s detallada acerca del libro ofertado.
- Cada p谩gina de la secci贸n de ofertas se guarda como archivo .csv para luego ser eliminados y unidos en uno solo, con nombre respectivo a la fecha de extracci贸n. Dicho proceso toma alrededor de 2 horas.

### 2. Limpieza de los datos extra铆dos

Se realiz贸 dentro de `limpieza_datos.py`, principalmente con uso de las librer铆as `pandas` y `numpy`. El proceso consiste en:

- Limpieza de variables num茅ricas y categ贸ricas. 
- Creaci贸n de variables, eliminaci贸n de valores irregulares, reasignaci贸n de categor铆as.
- Limpieza de variable de texto asociada a la descripci贸n del libro para su posterior uso en NLP.

### 3. Guardado de datos dentro de una base de datos local PostgreSQL

Se realiz贸 dentro de `guardar_en_SQL.py`, principalmente con uso de las librer铆as `pandas`, `sqlalchemy` y `psycopg2`. El proceso consiste en utilizar la variable engine definida dentro de `config.py` correspondiente a la direcci贸n de la base de datos creada en PostgreSQL.

### 4. An谩lisis predictivo de variables asociadas a un libro.

Se realizaron de manera paralela tres modelos predictivos para variables asociadas a los libros, separadas en tres distintos archivos. Dentro de cada archivo se ajustaron los datos acorde a la variable objetivo, para luego entrenar, guardar y testear los modelos. Los modelos entrenados fueron guardados dentro de la carpeta models. Los modelos son los siguientes:

- **Precio original**: Dentro del archivo `modelo_precio_original.py`, con objetivo de predecir el precio original de un libro, se ajust贸 un modelo de regresi贸n lineal.
- **Precio de oferta**: Dentro del archivo `modelo_precio_oferta.py`, con objetivo de predecir el precio de oferta de un libro, se ajust贸 un modelo de regresi贸n lineal.
- **Descuento**: Dentro del archivo `modelo_descuento.py`, con objetivo de predecir el porcentaje de descuento de un libro, separado en rangos de categor铆as, se ajust贸 un modelo de regresi贸n multinomial ordinal, con aplicaci贸n de t茅cnicas de remuestreo como SMOTE. Para este caso en particular, las predicciones no suelen ser muy precisas.

### 5. Uso de t茅cnicas de Procesamiento del Lenguaje Natural (NLP)

A煤n en construcci贸n y evaluando si es posible agregarlo al pipeline.

### 6. Creaci贸n de flujo de datos

Se realiz贸 dentro de `flow.py`, utilizando la librer铆a `prefect` para ello.
Dentro del archivo se definieron los pasos "task" y se juntaron dentro del flujo "flow". Siguiendo un orden de extracci贸n, limpieza, guardado y finalmente modelamiento de los datos.


## Apartados adicionales

Se incluir谩n los siguintes apartados a ser practicados, que bien pueden incorporarse despu茅s del paso de guardado de datos:
- Creaci贸n de API mediante FastAPI de manera local.
- An谩lisis descriptivo con visualizaci贸n de gr谩ficas por medio de PowerBI.
- Guardado en repositorio de GitHub.